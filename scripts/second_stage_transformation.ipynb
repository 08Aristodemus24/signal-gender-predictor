{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9057efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import io\n",
    "import librosa\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, FloatType, DoubleType, IntegerType\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from utilities.loaders import save_data_splits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f65cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `sparksession is none: typeerror: 'javapackage' object is not \n",
    "# callable` can be raised if the pyspark version being used is 4.0.0\n",
    "# which is not compatible to a python 3.11.8 version\n",
    "spark = SparkSession.builder.appName(\"app\")\\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\\\n",
    "    .config(\"spark.executor.memory\", \"4g\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.instances\", \"3\")\\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01826ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver/stage-01'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # cloud\n",
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-01\"\n",
    "# SILVER_DATA_PATH = os.path.join(URL.format(FOLDER_NAME=SILVER_FOLDER_NAME), SUB_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "\n",
    "# local\n",
    "DATA_DIR = \"../include/data/\"\n",
    "SILVER_FOLDER_NAME = \"silver/\"\n",
    "SUB_FOLDER_NAME = \"stage-01\"\n",
    "SILVER_DATA_DIR = os.path.join(DATA_DIR, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME)).replace(\"\\\\\", \"/\")\n",
    "SILVER_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0f89003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder_infos = dbutils.fs.ls(BRONZE_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b444eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signal_files(SPLIT_FOLDER):\n",
    "\n",
    "    # only include the parquet files without labels\n",
    "    signal_files = [\n",
    "        os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE).replace('\\\\', '/') \n",
    "        for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER)\n",
    "        # if file path has labels or .csv in it do not include in list\n",
    "        if (not \"labels\" in SIGNAL_DF_FILE) and (not \".csv\" in SIGNAL_DF_FILE)\n",
    "    ]\n",
    "\n",
    "    # print(signal_files)\n",
    "\n",
    "    signals_df = spark.read.format(\"parquet\").load(signal_files)\n",
    "\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df = read_signal_files(SILVER_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "605e4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-----+\n",
      "|      signals|           subjectId|rowId|\n",
      "+-------------+--------------------+-----+\n",
      "| -0.011077881|23yipikaye-201008...|    0|\n",
      "| -0.010467529|23yipikaye-201008...|    1|\n",
      "|  -0.00982666|23yipikaye-201008...|    2|\n",
      "| -0.010620117|23yipikaye-201008...|    3|\n",
      "| -0.009674072|23yipikaye-201008...|    4|\n",
      "| -0.006713867|23yipikaye-201008...|    5|\n",
      "|-0.0036621094|23yipikaye-201008...|    6|\n",
      "|-0.0025634766|23yipikaye-201008...|    7|\n",
      "|-0.0027770996|23yipikaye-201008...|    8|\n",
      "|-0.0034484863|23yipikaye-201008...|    9|\n",
      "|-0.0015869141|23yipikaye-201008...|   10|\n",
      "| 6.4086914E-4|23yipikaye-201008...|   11|\n",
      "| 0.0025024414|23yipikaye-201008...|   12|\n",
      "| 0.0030822754|23yipikaye-201008...|   13|\n",
      "| 0.0043945312|23yipikaye-201008...|   14|\n",
      "| 0.0038146973|23yipikaye-201008...|   15|\n",
      "| 0.0038452148|23yipikaye-201008...|   16|\n",
      "|  0.005554199|23yipikaye-201008...|   17|\n",
      "| 0.0049438477|23yipikaye-201008...|   18|\n",
      "| 0.0037231445|23yipikaye-201008...|   19|\n",
      "+-------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dc1033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signals_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.signals AS signals, \n",
    "        s.subjectId AS subjectId, \n",
    "        s.rowId AS rowId\n",
    "    FROM {train_labels_df} l\n",
    "    LEFT JOIN {signals_df} s\n",
    "    ON l.subjectId = s.subjectId\n",
    "\"\"\", signals_df=signals_df, train_labels_df=train_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d1a324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           subjectId|\n",
      "+--------------------+\n",
      "|  Aaron-20080318-kdl|\n",
      "| 1337ad-20170321-ajg|\n",
      "|Anniepoo-20140308...|\n",
      "|23yipikaye-201008...|\n",
      "|  Coren-20141121-pxp|\n",
      "| 1337ad-20170321-tkg|\n",
      "|Anniepoo-20140308...|\n",
      "|Anniepoo-20140308...|\n",
      "|   1028-20100710-hne|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df.select(\"subjectId\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38e33141",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_signals_flat_list = train_signals_df.where(F.col(\"subjectId\") == \"1028-20100710-hne\").select(\"signals\").rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e233ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 5))\n",
    "librosa.display.waveshow(np.array(subject_signals_flat_list), alpha=0.5, color=\"#8442f5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7259cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_signals_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.signals AS signals, \n",
    "        s.subjectId AS subjectId, \n",
    "        s.rowId AS rowId\n",
    "    FROM {val_labels_df} l\n",
    "    LEFT JOIN {signals_df} s\n",
    "    ON l.subjectId = s.subjectId\n",
    "\"\"\", signals_df=signals_df, val_labels_df=val_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70703746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           subjectId|\n",
      "+--------------------+\n",
      "| 1snoke-20120412-hge|\n",
      "|Anniepoo-20140308...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_signals_df.select(\"subjectId\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c7454349",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signals_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        s.signals AS signals, \n",
    "        s.subjectId AS subjectId, \n",
    "        s.rowId AS rowId\n",
    "    FROM {test_labels_df} l\n",
    "    LEFT JOIN {signals_df} s\n",
    "    ON l.subjectId = s.subjectId\n",
    "\"\"\", signals_df=signals_df, test_labels_df=test_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84ad1981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           subjectId|\n",
      "+--------------------+\n",
      "|Anniepoo-20140308...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_signals_df.select(\"subjectId\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @F.pandas_udf(returnType=FloatType(), functionType=F.PandasUDFType.GROUPED_AGG)\n",
    "# def get_peak_freq(segment: pd.Series):\n",
    "#     # calculate frequency domain features\n",
    "#     # get the spectrogram by calculating short time fourier transform\n",
    "#     spectrogram = np.abs(librosa.stft(segment))\n",
    "#     # print(f\"spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "#     # Get the frequencies corresponding to the spectrogram bins\n",
    "#     frequencies = librosa.fft_frequencies(sr=16000)\n",
    "#     # print(f\"frequencies shape: {frequencies.shape}\")\n",
    "\n",
    "#     # Find the frequency bin with the highest average energy\n",
    "#     peak_frequency_bin = np.argmax(np.mean(spectrogram, axis=1))\n",
    "\n",
    "#     # Get the peak frequency in Hz\n",
    "#     # calculate also peak frequency\n",
    "#     # I think dito na gagamit ng fast fourier transform\n",
    "#     # to obtain the frequency, or use some sort of function\n",
    "#     # to convert the raw audio signals into a spectogram\n",
    "#     peak_frequency = frequencies[peak_frequency_bin]\n",
    "\n",
    "#     return peak_frequency\n",
    "\n",
    "def extract_features(\n",
    "    signals_df: pyspark.sql.DataFrame,\n",
    "    # dataset: list[tuple[str, pyspark.sql.DataFrame]],\n",
    "    hertz: int=16000,\n",
    "    window_time: int=3,\n",
    "    hop_time: int=1):\n",
    "    \"\"\"\n",
    "    extracts the features from each segment of an audio signal\n",
    "\n",
    "    args:\n",
    "        dataset - \n",
    "        hertz - number of samples per second\n",
    "        window_time - number of seconds of the given window to consider\n",
    "        e.g. if number of seconds is 3 and hertz is 16000 or 16000\n",
    "        samples/rows per second then the window size we will consider\n",
    "        is 16000 * 3 or 48000\n",
    "        hop_time - seconds\n",
    "    \"\"\"\n",
    "    # we calculate the window size of each segment or the\n",
    "    # amount of samples it has to have based on the frequency\n",
    "    samples_per_win_size = int(window_time * hertz)\n",
    "    samples_per_hop_size = int(hop_time * hertz)\n",
    "    # print(f\"samples per window size: {samples_per_win_size}\")\n",
    "    # print(f\"samples per hop size: {samples_per_hop_size}\\n\")\n",
    "\n",
    "    \n",
    "    feat_window = Window.partitionBy(\"subjectId\").orderBy(\"rowId\").rowsBetween(Window.currentRow, samples_per_win_size - 1)\n",
    "    \n",
    "    signals_df = signals_df.withColumn(\"freq_skew\", F.skewness(\"signals\").over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_kurt\", F.kurtosis(\"signals\").over(feat_window))\n",
    "    \n",
    "    signals_df = signals_df.withColumn(\"freq_mean\", F.mean(\"signals\").over(feat_window))\n",
    "    # signals_df = signals_df.withColumn(\"freq_median\", F.median(\"signals\").over(feat_window))\n",
    "    # median over window function is not supported so we can use \n",
    "    signals_df = signals_df.withColumn(\"freq_median\", F.percentile(\"signals\", 0.5).over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_mode\", F.mode(\"signals\").over(feat_window))\n",
    "    \n",
    "    signals_df = signals_df.withColumn(\"freq_min\", F.min(\"signals\").over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_max\", F.max(\"signals\").over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_range\", F.col(\"freq_max\") - F.col(\"freq_min\"))\n",
    "    signals_df = signals_df.withColumn(\"freq_var\", F.variance(\"signals\").over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_std\", F.stddev(\"signals\").over(feat_window))\n",
    "    \n",
    "    signals_df = signals_df.withColumn(\"freq_first_quart\", F.percentile(\"signals\", 0.25).over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_third_quart\", F.percentile(\"signals\", 0.75).over(feat_window))\n",
    "    signals_df = signals_df.withColumn(\"freq_inter_quart_range\", F.col(\"freq_first_quart\") - F.col(\"freq_third_quart\"))\n",
    "\n",
    "    # signals_df = signals_df.withColumn(\"freq_peak\", get_peak_freq(F.col(\"signals\")).over(feat_window))\n",
    "    \n",
    "    # an implementation of the only including windows after a certain\n",
    "    # hop size, since we cannot do it directly using spark we can \n",
    "    # filter out the rows of windows that have not yet made the \n",
    "    # appropriate hop size using filtering \n",
    "    signals_df = signals_df.where((F.col(\"rowId\") % samples_per_hop_size) == 0)\n",
    "\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "173b4104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[signals: float, subjectId: string, rowId: int]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df.cache()\n",
    "val_signals_df.cache()\n",
    "test_signals_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9887315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|     signals|\n",
      "+------------+\n",
      "|1.5258789E-4|\n",
      "|1.5258789E-4|\n",
      "|9.1552734E-5|\n",
      "|9.1552734E-5|\n",
      "|6.1035156E-5|\n",
      "|9.1552734E-5|\n",
      "|9.1552734E-5|\n",
      "|9.1552734E-5|\n",
      "|9.1552734E-5|\n",
      "|6.1035156E-5|\n",
      "|3.0517578E-5|\n",
      "|3.0517578E-5|\n",
      "|6.1035156E-5|\n",
      "|3.0517578E-5|\n",
      "|3.0517578E-5|\n",
      "|         0.0|\n",
      "|         0.0|\n",
      "|3.0517578E-5|\n",
      "|3.0517578E-5|\n",
      "|6.1035156E-5|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df.select(\"signals\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f04b2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signals_df = extract_features(signals_df=train_signals_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3c34af99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[signals: float, subjectId: string, rowId: int, freq_skew: double, freq_kurt: double, freq_mean: double, freq_median: double, freq_mode: float, freq_min: float, freq_max: float, freq_range: float, freq_var: double, freq_std: double, freq_first_quart: double, freq_third_quart: double, freq_inter_quart_range: double]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5e468434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| rowId|\n",
      "+------+\n",
      "|     0|\n",
      "| 16000|\n",
      "| 32000|\n",
      "| 48000|\n",
      "| 64000|\n",
      "| 80000|\n",
      "| 96000|\n",
      "|112000|\n",
      "|128000|\n",
      "|144000|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df.select(\"rowId\").limit(10).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95be1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver/stage-02'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # cloud\n",
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-02\"\n",
    "# SILVER_DATA_PATH = os.path.join(URL.format(FOLDER_NAME=SILVER_FOLDER_NAME), SUB_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "\n",
    "# local\n",
    "DATA_DIR = \"../include/data/\"\n",
    "SILVER_FOLDER_NAME = \"silver/\"\n",
    "SUB_FOLDER_NAME = \"stage-02\"\n",
    "SILVER_DATA_DIR = os.path.join(DATA_DIR, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME)).replace(\"\\\\\", \"/\")\n",
    "SILVER_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7d404",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df.write\\\n",
    ".mode(\"overwrite\")\\\n",
    ".partitionBy(\"subjectId\")\\\n",
    ".option(\"compression\", \"snappy\")\\\n",
    ".parquet(os.path.join(SILVER_DATA_DIR, \"train\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "499a9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [(i, (i + 1) * 10, \"subject_1\") for i in range(20)]\n",
    "df_1 = spark.createDataFrame(sample_data, [\"row_id\", \"value\", \"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "be3110cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|row_id|value|subject_id|\n",
      "+------+-----+----------+\n",
      "|     0|   10| subject_1|\n",
      "|     1|   20| subject_1|\n",
      "|     2|   30| subject_1|\n",
      "|     3|   40| subject_1|\n",
      "|     4|   50| subject_1|\n",
      "|     5|   60| subject_1|\n",
      "|     6|   70| subject_1|\n",
      "|     7|   80| subject_1|\n",
      "|     8|   90| subject_1|\n",
      "|     9|  100| subject_1|\n",
      "|    10|  110| subject_1|\n",
      "|    11|  120| subject_1|\n",
      "|    12|  130| subject_1|\n",
      "|    13|  140| subject_1|\n",
      "|    14|  150| subject_1|\n",
      "|    15|  160| subject_1|\n",
      "|    16|  170| subject_1|\n",
      "|    17|  180| subject_1|\n",
      "|    18|  190| subject_1|\n",
      "|    19|  200| subject_1|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3c018fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [(i, (i + 1) * 10, \"subject_2\") for i in range(12)]\n",
    "df_2 = spark.createDataFrame(sample_data, [\"row_id\", \"value\", \"subject_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53f20662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|row_id|value|subject_id|\n",
      "+------+-----+----------+\n",
      "|     0|   10| subject_2|\n",
      "|     1|   20| subject_2|\n",
      "|     2|   30| subject_2|\n",
      "|     3|   40| subject_2|\n",
      "|     4|   50| subject_2|\n",
      "|     5|   60| subject_2|\n",
      "|     6|   70| subject_2|\n",
      "|     7|   80| subject_2|\n",
      "|     8|   90| subject_2|\n",
      "|     9|  100| subject_2|\n",
      "|    10|  110| subject_2|\n",
      "|    11|  120| subject_2|\n",
      "+------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5992d876",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: bigint, value: bigint, subject_id: string]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1.cache()\n",
    "df_2.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "719346f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_2.unionByName(df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42d402e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+\n",
      "|row_id|value|subject_id|\n",
      "+------+-----+----------+\n",
      "|     0|   10| subject_2|\n",
      "|     1|   20| subject_2|\n",
      "|     2|   30| subject_2|\n",
      "|     3|   40| subject_2|\n",
      "|     4|   50| subject_2|\n",
      "|     5|   60| subject_2|\n",
      "|     6|   70| subject_2|\n",
      "|     7|   80| subject_2|\n",
      "|     8|   90| subject_2|\n",
      "|     9|  100| subject_2|\n",
      "|    10|  110| subject_2|\n",
      "|    11|  120| subject_2|\n",
      "|     0|   10| subject_1|\n",
      "|     1|   20| subject_1|\n",
      "|     2|   30| subject_1|\n",
      "|     3|   40| subject_1|\n",
      "|     4|   50| subject_1|\n",
      "|     5|   60| subject_1|\n",
      "|     6|   70| subject_1|\n",
      "|     7|   80| subject_1|\n",
      "+------+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c74857d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_win_size = 6\n",
    "samples_per_hop_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "31e73bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_window = Window.partitionBy(\"subject_id\").orderBy(\"row_id\").rowsBetween(Window.currentRow, samples_per_win_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "415a6fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = df_3.withColumn(\"freq_std\", F.sum(\"value\").over(feat_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "81e70b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|row_id|value|subject_id|freq_std|\n",
      "+------+-----+----------+--------+\n",
      "|     0|   10| subject_1|     210|\n",
      "|     1|   20| subject_1|     270|\n",
      "|     2|   30| subject_1|     330|\n",
      "|     3|   40| subject_1|     390|\n",
      "|     4|   50| subject_1|     450|\n",
      "|     5|   60| subject_1|     510|\n",
      "|     6|   70| subject_1|     570|\n",
      "|     7|   80| subject_1|     630|\n",
      "|     8|   90| subject_1|     690|\n",
      "|     9|  100| subject_1|     750|\n",
      "|    10|  110| subject_1|     810|\n",
      "|    11|  120| subject_1|     870|\n",
      "|    12|  130| subject_1|     930|\n",
      "|    13|  140| subject_1|     990|\n",
      "|    14|  150| subject_1|    1050|\n",
      "|    15|  160| subject_1|     900|\n",
      "|    16|  170| subject_1|     740|\n",
      "|    17|  180| subject_1|     570|\n",
      "|    18|  190| subject_1|     390|\n",
      "|    19|  200| subject_1|     200|\n",
      "+------+-----+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b230ed",
   "metadata": {},
   "source": [
    "# an implementation of the only including windows after a certain hop size, since we cannot do it directly using spark we can filter out the rows of windows that have not yet made the appropriate hop size using filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e87f04ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond = ((F.col(\"row_id\") % samples_per_hop_size) == 0)\n",
    "df_3 = df_3.where(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "81eb24f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|row_id|value|subject_id|freq_std|\n",
      "+------+-----+----------+--------+\n",
      "|     0|   10| subject_1|     210|\n",
      "|     4|   50| subject_1|     450|\n",
      "|     8|   90| subject_1|     690|\n",
      "|    12|  130| subject_1|     930|\n",
      "|    16|  170| subject_1|     740|\n",
      "|     0|   10| subject_2|     210|\n",
      "|     4|   50| subject_2|     450|\n",
      "|     8|   90| subject_2|     420|\n",
      "+------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8326aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(row_id#2725L) AND ((row_id#2725L % 4) = 0))\n",
      "   +- Window [sum(value#2726L) windowspecdefinition(subject_id#2727, row_id#2725L ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), 5)) AS freq_std#2914L], [subject_id#2727], [row_id#2725L ASC NULLS FIRST]\n",
      "      +- Sort [subject_id#2727 ASC NULLS FIRST, row_id#2725L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(subject_id#2727, 200), ENSURE_REQUIREMENTS, [plan_id=2062]\n",
      "            +- Union\n",
      "               :- InMemoryTableScan [row_id#2725L, value#2726L, subject_id#2727]\n",
      "               :     +- InMemoryRelation [row_id#2725L, value#2726L, subject_id#2727], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :           +- *(1) Scan ExistingRDD[row_id#2725L,value#2726L,subject_id#2727]\n",
      "               +- InMemoryTableScan [row_id#2706L, value#2707L, subject_id#2708]\n",
      "                     +- InMemoryRelation [row_id#2706L, value#2707L, subject_id#2708], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) Scan ExistingRDD[row_id#2706L,value#2707L,subject_id#2708]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "80e4e89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[row_id: bigint, value: bigint, subject_id: string, freq_std: bigint]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12c703a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Filter (isnotnull(row_id#2725L) AND ((row_id#2725L % 4) = 0))\n",
      "   +- Window [sum(value#2726L) windowspecdefinition(subject_id#2727, row_id#2725L ASC NULLS FIRST, specifiedwindowframe(RowFrame, currentrow$(), 5)) AS freq_std#2914L], [subject_id#2727], [row_id#2725L ASC NULLS FIRST]\n",
      "      +- Sort [subject_id#2727 ASC NULLS FIRST, row_id#2725L ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(subject_id#2727, 200), ENSURE_REQUIREMENTS, [plan_id=2062]\n",
      "            +- Union\n",
      "               :- InMemoryTableScan [row_id#2725L, value#2726L, subject_id#2727]\n",
      "               :     +- InMemoryRelation [row_id#2725L, value#2726L, subject_id#2727], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "               :           +- *(1) Scan ExistingRDD[row_id#2725L,value#2726L,subject_id#2727]\n",
      "               +- InMemoryTableScan [row_id#2706L, value#2707L, subject_id#2708]\n",
      "                     +- InMemoryRelation [row_id#2706L, value#2707L, subject_id#2708], StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "                           +- *(1) Scan ExistingRDD[row_id#2706L,value#2707L,subject_id#2708]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "489f0473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----------+--------+\n",
      "|row_id|value|subject_id|freq_std|\n",
      "+------+-----+----------+--------+\n",
      "|     0|   10| subject_1|     210|\n",
      "|     4|   50| subject_1|     450|\n",
      "|     8|   90| subject_1|     690|\n",
      "|    12|  130| subject_1|     930|\n",
      "|    16|  170| subject_1|     740|\n",
      "|     0|   10| subject_2|     210|\n",
      "|     4|   50| subject_2|     450|\n",
      "|     8|   90| subject_2|     420|\n",
      "+------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c18d72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
