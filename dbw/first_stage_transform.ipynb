{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1a92e0-fe2a-44d6-9a12-fe3f8524c819",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import io\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, FloatType, DoubleType, IntegerType\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80ba5ea9-0e29-49bd-b917-33e6050b70fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# `sparksession is none: typeerror: 'javapackage' object is not \n",
    "# callable` can be raised if the pyspark version being used is 4.0.0\n",
    "# which is not compatible to a python 3.11.8 version\n",
    "spark = SparkSession.builder.appName(\"app\").getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"14g\")\\\n",
    "    # .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")\\\n",
    "    # .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6643cd3c-e0b7-4146-a488-84d84b20d87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# BRONZE_FOLDER_NAME = \"sgppipelinesa-bronze\"\n",
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# BRONZE_DATA_PATH = URL.format(FOLDER_NAME=BRONZE_FOLDER_NAME)\n",
    "# BRONZE_DATA_PATH\n",
    "# folder_infos = dbutils.fs.ls(BRONZE_DATA_PATH)\n",
    "BRONZE_FOLDER_NAME = \"bronze/\"\n",
    "DATA_PATH = \"../include/data/\"\n",
    "BRONZE_DATA_PATH = os.path.join(DATA_PATH, BRONZE_FOLDER_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608751f0-344b-467b-9672-45433aa9ecc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample_folder = folder_infos[-1].path\n",
    "# sample_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8829f80-e22c-4a4a-9aa1-1381ab00f8d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# sample_folder.strip('/').split('/')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e982f28-f272-4576-91d9-b07ec18c3ffa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# type(folder_infos[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1e55de7-a37a-4e39-8889-bf57f0092284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.ls(folder_infos[-1].path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1028-20100710-hne',\n",
       " '1337ad-20170321-ajg',\n",
       " '1337ad-20170321-tkg',\n",
       " '1snoke-20120412-hge',\n",
       " '23yipikaye-20100807-ujm',\n",
       " 'Aaron-20080318-kdl',\n",
       " 'Anniepoo-20140308-bft',\n",
       " 'Anniepoo-20140308-cqj',\n",
       " 'Anniepoo-20140308-fcp',\n",
       " 'Anniepoo-20140308-hns',\n",
       " 'Anniepoo-20140308-nky',\n",
       " 'Coren-20141121-pxp']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_infos = os.listdir(BRONZE_DATA_PATH)\n",
    "file_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f978569b-1c1f-4d9e-a385-5c1bd68873e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# labels_df = spark.read.format('text')\\\n",
    "#     .option(\"lineSep\", \"\\n\")\\\n",
    "#     .load(os.path.join(BRONZE_DATA_PATH, \"1337ad-20170321-ajg\", \"etc\", \"README\"))\n",
    "# labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|    User Name:1337ad|\n",
      "|                    |\n",
      "|Speaker Character...|\n",
      "|                    |\n",
      "|      Gender: Female|\n",
      "|    Age Range: Adult|\n",
      "|        Language: EN|\n",
      "|Pronunciation dia...|\n",
      "|                    |\n",
      "|Recording Informa...|\n",
      "|                    |\n",
      "|Microphone make: n/a|\n",
      "|Microphone type: ...|\n",
      "|Audio card make: ...|\n",
      "|Audio card type: ...|\n",
      "|Audio Recording S...|\n",
      "|                O/S:|\n",
      "|                    |\n",
      "|          File Info:|\n",
      "|                    |\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_df = spark.read.format(\"text\")\\\n",
    "    .option(\"lineSep\", \"\\n\")\\\n",
    "    .load([os.path.join(BRONZE_DATA_PATH, file_info, \"etc\", \"README\") for file_info in file_infos])\n",
    "# labels_df = spark.read.format(\"text\")\\\n",
    "#     .option(\"wholeText\", \"true\")\\\n",
    "#     .load([os.path.join(BRONZE_DATA_PATH, file_info, \"etc\", \"README\") for file_info in file_infos])\n",
    "labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df.withColumn(\"filePath\", F.input_file_name()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee5e370f-9202-4f52-bfda-5ff0c7095ef9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# labels_df.withColumn(\"filePath\", F.input_file_name()).where(\n",
    "#     F.lower(F.col(\"value\")).contains(\"gender\")\n",
    "# ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|            filePath|\n",
      "+--------------------+--------------------+\n",
      "|    User Name:1337ad|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "|Speaker Character...|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "|      Gender: Female|file:///c:/Users/...|\n",
      "|    Age Range: Adult|file:///c:/Users/...|\n",
      "|        Language: EN|file:///c:/Users/...|\n",
      "|Pronunciation dia...|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "|Recording Informa...|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "|Microphone make: n/a|file:///c:/Users/...|\n",
      "|Microphone type: ...|file:///c:/Users/...|\n",
      "|Audio card make: ...|file:///c:/Users/...|\n",
      "|Audio card type: ...|file:///c:/Users/...|\n",
      "|Audio Recording S...|file:///c:/Users/...|\n",
      "|                O/S:|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "|          File Info:|file:///c:/Users/...|\n",
      "|                    |file:///c:/Users/...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labels_df = labels_df.withColumn(\"filePath\", F.input_file_name())\n",
    "labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df.where(F.lower(F.col(\"value\")).contains(\"gender\"))\n",
    "# labels_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df.withColumn(\n",
    "    \"value\", \n",
    "    # extract only the gender of the subject in meta data\n",
    "    F.regexp_replace(\n",
    "        F.lower(F.col(\"value\")), \n",
    "        r\"(gender)|[:;\\[\\]\\t\\n\\s]+\", \n",
    "        \"\"\n",
    "    )\n",
    ")\n",
    "# labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df.withColumn(\n",
    "    \"value\",\n",
    "    # sometimes the gender may be in a different language\n",
    "    # e.g. the 'male' in german may have the string start\n",
    "    # with 'mä' so we should return male if this is the case\n",
    "    # and vice versa for females translated to a different\n",
    "    # language \n",
    "    F.when(\n",
    "        F.col(\"value\").startswith(\"ma\") | F.col(\"value\").startswith(\"mä\"),\n",
    "        \"male\"\n",
    "    ).when(\n",
    "        F.col(\"value\").startswith(\"fem\") | F.col(\"value\").startswith(\"wei\"),\n",
    "        \"female\"\n",
    "    ).otherwise(\n",
    "        \"unknown\"\n",
    "    )\n",
    ")\n",
    "# labels_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clean filePath column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df.withColumn(\n",
    "#     \"subjectId\",\n",
    "#     F.element_at(\n",
    "#         # splits the filepath from 'file:///c:/Users/LARRY/Documents/Scripts/.../bronze/1337ad-20170321-ajg/etc/README\n",
    "#         # to array of the directory tree of the files path e.g. \n",
    "#         # ['file:', ..., 'Scripts', ..., 'bronze', '<subject id>, 'etc', 'readme']\n",
    "#         # so in order to extract subject id or the file name we have to \n",
    "#         # get the 3rd to the last element\n",
    "#         F.split(\n",
    "#             F.col(\"filepath\"),\n",
    "#             r\"\\/\"\n",
    "#         ),\n",
    "#         -3\n",
    "#     )\n",
    "# ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = labels_df.withColumn(\n",
    "    \"subjectId\",\n",
    "    F.element_at(\n",
    "        # splits the filepath from 'file:///c:/Users/LARRY/Documents/Scripts/.../bronze/1337ad-20170321-ajg/etc/README\n",
    "        # to array of the directory tree of the files path e.g. \n",
    "        # ['file:', ..., 'Scripts', ..., 'bronze', '<subject id>, 'etc', 'readme']\n",
    "        # so in order to extract subject id or the file name we have to \n",
    "        # get the 3rd to the last element\n",
    "        F.split(\n",
    "            F.col(\"filepath\"),\n",
    "            r\"\\/\"\n",
    "        ),\n",
    "        -3\n",
    "    )\n",
    ")\n",
    "# labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string, filePath: string, subjectId: string]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_labels_df = labels_df.where(F.col(\"value\") == \"male\")\n",
    "# male_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "female_labels_df = labels_df.where(F.col(\"value\") == \"female\")\n",
    "# female_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string, filePath: string, subjectId: string]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "male_labels_df.persist()\n",
    "female_labels_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_male_labels_df, val_male_labels_df, test_male_labels_df = male_labels_df.randomSplit(weights=[0.7, 0.15, 0.15], seed=0)\n",
    "# train_male_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_male_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_male_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_female_labels_df, val_female_labels_df, test_female_labels_df = female_labels_df.randomSplit(weights=[0.7, 0.15, 0.15], seed=0)\n",
    "# train_female_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_female_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_female_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string, filePath: string, subjectId: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_male_labels_df.persist()\n",
    "val_male_labels_df.persist()\n",
    "test_male_labels_df.persist()\n",
    "train_female_labels_df.persist()\n",
    "val_female_labels_df.persist()\n",
    "test_female_labels_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = train_male_labels_df.unionByName(train_female_labels_df)\n",
    "# train_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels_df = val_male_labels_df.unionByName(val_female_labels_df)\n",
    "# val_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels_df = test_male_labels_df.unionByName(test_female_labels_df)\n",
    "# test_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40d407e5-25a8-4955-a794-09e500ea08e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # what I want to do is list the files get all the file names in the bronze \n",
    "# # container, which will give me a list, and then use that list to concurrently\n",
    "# # list the files inside these list of directories\n",
    "# def load_labels(DIR, folder_infos):\n",
    "#     def helper(folder_info):\n",
    "#         try:\n",
    "#             # remove trailing backslash\n",
    "#             folder = folder_info.path.strip('/').split('/')[-1]\n",
    "#             file_path = os.path.join(DIR, folder, \"etc\", \"README\")\n",
    "            \n",
    "#             print(file_path)\n",
    "#             with open(file_path, \"r\") as file:\n",
    "#                 lines = [line for line in file.readlines() if \"gender\" in line.lower()]\n",
    "#                 file.close()\n",
    "\n",
    "#             print(lines)\n",
    "\n",
    "#             # extract only the gender of the subject in meta data\n",
    "#             # print(lines[0].lower())\n",
    "#             string = re.sub(r\"(gender)\", \"\", lines[0].lower())\n",
    "#             string = re.sub(r\"[:;\\[\\]\\t\\n\\s]\", \"\", string)\n",
    "\n",
    "#             if string:\n",
    "#                 gender = string\n",
    "#                 if gender.startswith(\"ma\") or gender.startswith(\"mä\"):\n",
    "#                     return folder, string, \"male\"\n",
    "#                 elif gender.startswith(\"fem\") or gender.startswith(\"wei\"):\n",
    "#                     return folder, string, \"female\"\n",
    "#                 else:\n",
    "#                     return folder, string, \"unknown\"\n",
    "            \n",
    "#         except IndexError:\n",
    "#             return folder, \"unknown\", \"unknown\"\n",
    "        \n",
    "#         except FileNotFoundError:\n",
    "#             return folder, \"unknown\", \"unknown\"\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=5) as exe:\n",
    "#         subjects_labels = list(exe.map(helper, folder_infos))\n",
    "        \n",
    "        \n",
    "#     return subjects_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_audio(DIR: str, folders: list, hertz=16000):\n",
    "#     \"\"\"\n",
    "#     loads audio signals from each .wav file of each subject\n",
    "#     \"\"\"\n",
    "\n",
    "#     def helper(folder):\n",
    "#     # for folder in folders:\n",
    "#         try:\n",
    "#             wavs_dir = os.path.join(DIR, folder, \"wav\")\n",
    "#             path_to_wavs = os.listdir(wavs_dir)\n",
    "\n",
    "#         # this is if a .wav file is not used as a directory so \n",
    "#         # try flac \n",
    "#         except FileNotFoundError:\n",
    "#             wavs_dir = os.path.join(DIR, folder, \"flac\")\n",
    "#             path_to_wavs = os.listdir(wavs_dir)\n",
    "\n",
    "#         finally:\n",
    "#             # create storage for list of signals to all be \n",
    "#             # concatenated later\n",
    "#             ys = []\n",
    "\n",
    "#             # create figure, and axis\n",
    "#             # fig, axes = plt.subplots(nrows=len(path_to_wavs), ncols=1, figsize=(12, 30))\n",
    "            \n",
    "#             for index, wav in enumerate(path_to_wavs):\n",
    "\n",
    "#                 wav_path = os.path.join(wavs_dir, wav)\n",
    "#                 # print(wav_path)\n",
    "\n",
    "#                 # each .wav file has a sampling frequency is 16000 hertz \n",
    "#                 y, sr = librosa.load(wav_path, sr=hertz)\n",
    "\n",
    "#                 # audio recordings can have different length\n",
    "#                 print(f\"shape of audio signals {y.shape}\")\n",
    "#                 print(f\"sampling rate of audio signals after interpolation: {sr}\")\n",
    "\n",
    "#                 # top_db is set to 20 representing any signal below\n",
    "#                 # 20 decibels will be considered silence\n",
    "#                 y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "#                 # append y to ys \n",
    "#                 ys.extend(y_trimmed.tolist())\n",
    "\n",
    "#             # # concatenate all audio signals into one final signal as \n",
    "#             # # this is all anyway recorded in the voice of the same gender\n",
    "#             # final = np.concatenate(ys, axis=0)\n",
    "#             # print(f\"shape of final signal: {final.shape}\")\n",
    "\n",
    "#             # print(f\"shape of signal: {y.shape}\")\n",
    "#             # print(f\"shape of trimmed signal: {y_trimmed.shape}\")\n",
    "#             # print(f\"sampling rate: {sr}\")\n",
    "#             # librosa.display.waveshow(final, alpha=0.5)\n",
    "\n",
    "#             # plt.tight_layout()\n",
    "#             # plt.show()\n",
    "\n",
    "#             return folder, ys\n",
    "        \n",
    "#     # concurrently load .wav files and trim  each .wav files\n",
    "#     # audio signal and combine into one signal for each subject \n",
    "#     with ThreadPoolExecutor(max_workers=5) as exe:\n",
    "#         signals = list(exe.map(helper, folders))\n",
    "        \n",
    "#     return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals = load_audio(BRONZE_DATA_PATH, file_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a female voice\n",
    "# signals[7][0], signals[2][0], signals[8][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this is a male voice\n",
    "# signals[0][0], signals[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(17, 5))\n",
    "# librosa.display.waveshow(np.array(signals[0][-1]), alpha=0.5, color=\"#8442f5\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snoke = list(filter(lambda datum: datum[0] == \"1337ad-20170321-ajg\", signals))[-1]\n",
    "# len(snoke[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading audio signals\n",
    "need some way to concurrently build this array:\n",
    "`[\"url/<subject k>/wav/*.wav\", \"url/<subject k>/wav/*.wav\", \"url/<subject k>/flac/*.flac\"]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to load audio with librosa\n",
    "@F.udf(returnType=ArrayType(FloatType()))\n",
    "def load_audio_with_librosa(content):\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Create a file-like object from the binary content \n",
    "        # which spark.read.format(\"binaryFile\").load(\"<path>\")\n",
    "        # returns\n",
    "        audio_buffer = io.BytesIO(content)\n",
    "\n",
    "        # we convert this audio buffer array as audio using librosa\n",
    "        # sr=None to preserve original sample rate\n",
    "        y, sr = librosa.load(audio_buffer, sr=16000) \n",
    "        \n",
    "        # top_db is set to 20 representing any signal below\n",
    "        # 20 decibels will be considered silence\n",
    "        y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "        # Convert numpy array to list for Spark dataframe\n",
    "        return y_trimmed.tolist()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_audio(DIR: str, folders: list, hertz=16000):\n",
    "    \"\"\"\n",
    "    loads audio signals from each .wav file of each subject\n",
    "    \"\"\"\n",
    "\n",
    "    def helper(folder):\n",
    "        \"\"\"\n",
    "        collects all the paths of wav files of each subject and\n",
    "        forms a wildcard out of it that a spark session can use\n",
    "        to concurrent read the files into a spark dataframe \n",
    "        \"\"\"\n",
    "        \n",
    "        # for folder in folders:\n",
    "        try:\n",
    "            subject_wav_paths = os.path.join(DIR, folder, \"wav\", \"*.wav\")\n",
    "\n",
    "        # this is if a .wav file is not used as a directory so \n",
    "        # try flac \n",
    "        except FileNotFoundError:\n",
    "            subject_wav_paths = os.path.join(DIR, folder, \"flac\", \"*.flac\")\n",
    "\n",
    "        finally:\n",
    "            signals_df = spark.read.format(\"binaryFile\").load(subject_wav_paths)\n",
    "            signals_df = signals_df.drop(*[\"modificationTime\", \"length\"])\n",
    "            signals_df = signals_df.withColumn(\n",
    "                \"subjectId\",\n",
    "                F.element_at(\n",
    "                    # splits the filepath from 'file:///c:/Users/LARRY/Documents/Scripts/.../bronze/1337ad-20170321-ajg/etc/README\n",
    "                    # to array of the directory tree of the files path e.g. \n",
    "                    # ['file:', ..., 'Scripts', ..., 'bronze', '<subject id>, 'etc', 'readme']\n",
    "                    # so in order to extract subject id or the file name we have to \n",
    "                    # get the 3rd to the last element\n",
    "                    F.split(\n",
    "                        F.col(\"path\"),\n",
    "                        r\"\\/\"\n",
    "                    ),\n",
    "                    -3\n",
    "                )\n",
    "            )\n",
    "            # we convert the binary data to signals using librosa \n",
    "            signals_df = signals_df.withColumn(\"signals\", load_audio_with_librosa(\"content\"))\n",
    "\n",
    "            # we group the list column using a collect_list() aggregator\n",
    "            # and then flatten this list of lists resulting from\n",
    "            # this aggregator using flatten() \n",
    "            signals_df = signals_df.groupBy(\"subjectId\").agg(\n",
    "                # group according to subject id because we are grouping\n",
    "                # a list column we will use collect list which will \n",
    "                # result in unflattened lists of lists that's why we need\n",
    "                # a secondary flatten function \n",
    "                F.flatten(F.collect_list(F.col(\"signals\"))).alias(\"signals\")\n",
    "            )\n",
    "\n",
    "            return folder, signals_df\n",
    "        \n",
    "    # concurrently load .wav files and trim  each .wav files\n",
    "    # audio signal and combine into one signal for each subject \n",
    "    with ThreadPoolExecutor(max_workers=5) as exe:\n",
    "        signals = list(exe.map(helper, folders))\n",
    "        \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df = load_audio(BRONZE_DATA_PATH, file_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('1337ad-20170321-ajg', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('1337ad-20170321-tkg', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('1snoke-20120412-hge', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('23yipikaye-20100807-ujm',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Aaron-20080318-kdl', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-bft',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-cqj',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-fcp',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-hns',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-nky',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Coren-20141121-pxp', DataFrame[subjectId: string, signals: array<float>])]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# signals_df[0][-1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_signals_df = spark.read.format(\"binaryFile\").load(os.path.join(BRONZE_DATA_PATH, file_infos[0], \"wav\", \"*.wav\"))\n",
    "# test_signals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_content = test_signals_df.limit(1).collect()[-1].content\n",
    "# test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_stream = io.BytesIO(test_content)\n",
    "# test_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# librosa.load(test_stream, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects = train_labels_df.select(\"subjectId\").rdd.map(lambda r: r[0]).collect()\n",
    "val_subjects = val_labels_df.select(\"subjectId\").rdd.map(lambda r: r[0]).collect()\n",
    "test_subjects = test_labels_df.select(\"subjectId\").rdd.map(lambda r: r[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aaron-20080318-kdl',\n",
       " '1snoke-20120412-hge',\n",
       " '1028-20100710-hne',\n",
       " '23yipikaye-20100807-ujm',\n",
       " '1337ad-20170321-tkg',\n",
       " 'Anniepoo-20140308-bft',\n",
       " 'Anniepoo-20140308-cqj',\n",
       " 'Anniepoo-20140308-hns',\n",
       " 'Anniepoo-20140308-nky']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('1337ad-20170321-tkg', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('1snoke-20120412-hge', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('23yipikaye-20100807-ujm',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Aaron-20080318-kdl', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-bft',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-cqj',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-hns',\n",
       "  DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Anniepoo-20140308-nky',\n",
       "  DataFrame[subjectId: string, signals: array<float>])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df = list(filter(lambda subject: subject[0] in train_subjects, signals_df))\n",
    "train_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1337ad-20170321-ajg', DataFrame[subjectId: string, signals: array<float>]),\n",
       " ('Coren-20141121-pxp', DataFrame[subjectId: string, signals: array<float>])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_signals_df = list(filter(lambda subject: subject[0] in val_subjects, signals_df))\n",
    "val_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Anniepoo-20140308-fcp',\n",
       "  DataFrame[subjectId: string, signals: array<float>])]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_signals_df = list(filter(lambda subject: subject[0] in test_subjects, signals_df))\n",
    "test_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_splits(df: list[tuple[str, pyspark.sql.DataFrame]] | pyspark.sql.DataFrame, split: str, type_: str, save_path: str):\n",
    "    \"\"\"\n",
    "    saves the dataframe into different folders in a silver\n",
    "    staging layer representing the train, validation, and \n",
    "    test splits\n",
    "\n",
    "    args:\n",
    "        df - a list of tuples with pairs of the subject name\n",
    "        and its respective spark dataframe representing its\n",
    "        signals\n",
    "\n",
    "        split - a string that tells the function what folder\n",
    "        should the data split with dataframes be saved. Can\n",
    "        either be 'train', 'validate', or 'test' \n",
    "\n",
    "        type - a string that tells the function if the dataframe\n",
    "        to be saved is the labels or the signals. Can either\n",
    "        be 'labels' or 'signals'\n",
    "\n",
    "        save_path - a string that tells this function where to\n",
    "        save the dataframe\n",
    "    \"\"\"\n",
    "    # make a directory based on the data split specified\n",
    "    # e.g. train, validate, test\n",
    "    os.makedirs(os.path.join(save_path, split), exist_ok=True)\n",
    "    \n",
    "    if type_.lower() == \"signals\":\n",
    "        def helper(subject_signal_df):\n",
    "            subject_id, signal_df = subject_signal_df\n",
    "            file_name = f\"{save_path}/{split}/{subject_id}_signals.parquet\"\n",
    "            signal_df.write.mode(\"overwrite\").parquet(file_name)\n",
    "        \n",
    "        # loop through each subjects signal dataframe\n",
    "        # concurrently\n",
    "        with ThreadPoolExecutor(max_workers=5) as exe:\n",
    "            exe.map(helper, df)\n",
    "\n",
    "    elif type_.lower() == \"labels\":\n",
    "        file_name = f\"{save_path}/{split}/labels.parquet\"\n",
    "        df.write.mode(\"overwrite\").parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver/'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SILVER_DATA_PATH = URL.format(FOLDER_NAME=SILVER_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "SILVER_FOLDER_NAME = \"silver/\"\n",
    "SILVER_DATA_PATH = os.path.join(DATA_PATH, SILVER_FOLDER_NAME)\n",
    "SILVER_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_splits(train_signals_df, split=\"train\", type_=\"signals\", save_path=SILVER_DATA_PATH)\n",
    "save_data_splits(val_signals_df, split=\"validate\", type_=\"signals\", save_path=SILVER_DATA_PATH)\n",
    "save_data_splits(test_signals_df, split=\"test\", type_=\"signals\", save_path=SILVER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_splits(train_labels_df, split=\"train\", type_=\"labels\", save_path=SILVER_DATA_PATH)\n",
    "save_data_splits(train_labels_df, split=\"validate\", type_=\"labels\", save_path=SILVER_DATA_PATH)\n",
    "save_data_splits(train_labels_df, split=\"test\", type_=\"labels\", save_path=SILVER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_signals_flat = train_signals_df[0][-1].withColumn(\"signals\", F.explode(F.col(\"signals\")))\n",
    "# subject_signals_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_signals_flat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_signals_flat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_signals_flat_list = subject_signals_flat.select(\"signals\").rdd.map(lambda r: r[0]).collect()\n",
    "# subject_signals_flat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(17, 5))\n",
    "# librosa.display.waveshow(np.array(subject_signals_flat_list), alpha=0.5, color=\"#8442f5\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_df.unpersist()\n",
    "# male_labels_df.unpersist()\n",
    "# female_labels_df.unpersist()\n",
    "# train_male_labels_df.unpersist()\n",
    "# val_male_labels_df.unpersist()\n",
    "# test_male_labels_df.unpersist()\n",
    "# train_female_labels_df.unpersist()\n",
    "# val_female_labels_df.unpersist()\n",
    "# test_female_labels_df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ingest-data",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
