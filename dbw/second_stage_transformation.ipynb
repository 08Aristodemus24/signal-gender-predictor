{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9057efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import io\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, FloatType, DoubleType, IntegerType\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f65cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `sparksession is none: typeerror: 'javapackage' object is not \n",
    "# callable` can be raised if the pyspark version being used is 4.0.0\n",
    "# which is not compatible to a python 3.11.8 version\n",
    "spark = SparkSession.builder.appName(\"app\").getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"14g\")\\\n",
    "    # .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")\\\n",
    "    # .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01826ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver\\\\stage-01'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-01\"\n",
    "# SILVER_DATA_PATH = os.path.join(URL.format(FOLDER_NAME=SILVER_FOLDER_NAME), SUB_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "# folder_infos = dbutils.fs.ls(BRONZE_DATA_PATH)\n",
    "\n",
    "DATA_PATH = \"../include/data/\"\n",
    "SILVER_FOLDER_NAME = \"silver\"\n",
    "SUB_FOLDER_NAME = \"stage-01\"\n",
    "SILVER_DATA_PATH = os.path.join(DATA_PATH, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME))\n",
    "SILVER_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e875b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"train\", \"labels.parquet\"))\n",
    "val_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"validate\", \"labels.parquet\"))\n",
    "test_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"test\", \"labels.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a1956c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| value|            filePath|           subjectId|\n",
      "+------+--------------------+--------------------+\n",
      "|  male|file:///c:/Users/...|23yipikaye-201008...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...| 1337ad-20170321-tkg|\n",
      "|  male|file:///c:/Users/...| 1snoke-20120412-hge|\n",
      "|  male|file:///c:/Users/...|  Aaron-20080318-kdl|\n",
      "|  male|file:///c:/Users/...|   1028-20100710-hne|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3449b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "| value|            filePath|          subjectId|\n",
      "+------+--------------------+-------------------+\n",
      "|female|file:///c:/Users/...|1337ad-20170321-ajg|\n",
      "|  male|file:///c:/Users/...| Coren-20141121-pxp|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2f1555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| value|            filePath|           subjectId|\n",
      "+------+--------------------+--------------------+\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_labels_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b467ef",
   "metadata": {},
   "source": [
    "#### we will load the data in this format for concurrent processing and to prevent bottle neck issues of having to read files into one dataframe and then just to only convert it back to a array fo tuples with subject name and a corresponding spark dataframe. So why not instead read the parquet files like this?\n",
    "```\n",
    "[\n",
    "  (<subject 1>, <subject 1 spark df>),\n",
    "  (<subject 2>, <subject 2 spark df>),\n",
    "  (<subject 3>, <subject 3 spark df>),\n",
    "  ...\n",
    "  (<subject n>, <subject n spark df>),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b444eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signal_files(SPLIT_FOLDER):\n",
    "\n",
    "    # only include the parquet files without labels\n",
    "    signal_files = [\n",
    "        os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE).replace('\\\\', '/') for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER) if not \"labels\" in SIGNAL_DF_FILE]\n",
    "\n",
    "    def helper(signal_file):\n",
    "        subject_id = signal_file.split('/')[-1].replace(\"_signals.parquet\", \"\")\n",
    "        signal_df = spark.read.format(\"parquet\").load(signal_file)\n",
    "        return subject_id, signal_df\n",
    "\n",
    "    with ThreadPoolExecutor() as exe:\n",
    "        signals_df = list(exe.map(helper, signal_files))\n",
    "\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9bf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_URL = os.path.join(SILVER_DATA_PATH, \"{SPLIT}\")\n",
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"train\")\n",
    "train_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "910f5493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('1337ad-20170321-tkg',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('1snoke-20120412-hge',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('23yipikaye-20100807-ujm',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Aaron-20080318-kdl', DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-bft',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-cqj',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-hns',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-nky',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "093b15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"validate\")\n",
    "val_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f6e6b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1337ad-20170321-ajg',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Coren-20141121-pxp', DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "175ea18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"test\")\n",
    "test_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "729c890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---+\n",
      "|        subjectId|      signals| ID|\n",
      "+-----------------+-------------+---+\n",
      "|1028-20100710-hne|-0.0054626465|  1|\n",
      "|1028-20100710-hne|-0.0053710938|  2|\n",
      "|1028-20100710-hne|-0.0055236816|  3|\n",
      "|1028-20100710-hne| -0.005493164|  4|\n",
      "|1028-20100710-hne| -0.005340576|  5|\n",
      "|1028-20100710-hne|-0.0047302246|  6|\n",
      "|1028-20100710-hne| -0.004486084|  7|\n",
      "|1028-20100710-hne|-0.0047302246|  8|\n",
      "|1028-20100710-hne|-0.0049438477|  9|\n",
      "|1028-20100710-hne| -0.004760742| 10|\n",
      "|1028-20100710-hne|-0.0042419434| 11|\n",
      "|1028-20100710-hne| -0.004211426| 12|\n",
      "|1028-20100710-hne| -0.004119873| 13|\n",
      "|1028-20100710-hne|-0.0032348633| 14|\n",
      "|1028-20100710-hne|-0.0029907227| 15|\n",
      "|1028-20100710-hne|-0.0032958984| 16|\n",
      "|1028-20100710-hne|-0.0035095215| 17|\n",
      "|1028-20100710-hne|-0.0036621094| 18|\n",
      "|1028-20100710-hne|-0.0036621094| 19|\n",
      "|1028-20100710-hne| -0.003967285| 20|\n",
      "+-----------------+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df[0][-1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ccd87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(dataset: list[tuple[str, pyspark.sql.DataFrame]], hertz: int=16000, window_time: int=3, hop_time: int=1):\n",
    "    \"\"\"\n",
    "    extracts the features from each segment of an audio signal\n",
    "    \"\"\"\n",
    "    \n",
    "    def helper(datum):\n",
    "        # we access the SCR values via raw data column\n",
    "        subject_id = datum[0]\n",
    "        signal_df = datum[1]\n",
    "\n",
    "        # # get number of rows of 16000hz signals \n",
    "        # n_rows = x_signals.shape[0]\n",
    "        # # print(n_rows)\n",
    "\n",
    "        # we calculate the window size of each segment or the\n",
    "        # amount of samples it has to have based on the frequency\n",
    "        samples_per_win_size = int(window_time * hertz)\n",
    "        samples_per_hop_size = int(hop_time * hertz)\n",
    "        # print(f\"samples per window size: {samples_per_win_size}\")\n",
    "        # print(f\"samples per hop size: {samples_per_hop_size}\\n\")\n",
    "\n",
    "        \n",
    "        feat_window = Window.orderBy(\"ID\").rowsBetween(-(samples_per_win_size - 1), Window.currentRow)\n",
    "        signal_df = signal_df.withColumn(\"freq_std\", F.stddev(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_skew\", F.skewness(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_kurt\", F.kurtosis(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_mean\", F.mean(\"signals\").over(feat_window))\n",
    "        # signal_df = signal_df.withColumn(\"freq_median\", F.median(\"signals\").over(feat_window))\n",
    "        # median over window function is not supported so we can use \n",
    "        signal_df = signal_df.withColumn(\"freq_median\", F.percentile(\"signals\", 0.5).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_mode\", F.mode(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_min\", F.min(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_max\", F.max(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_var\", F.variance(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_first_quart\", F.percentile(\"signals\", 0.25).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_third_quart\", F.percentile(\"signals\", 0.75).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_inter_quart_range\", F.col(\"freq_first_quart\") - F.col(\"freq_third_quart\"))\n",
    "        \n",
    "        \n",
    "\n",
    "        # range\n",
    "        # interquartile range\n",
    "        # variance  \n",
    "\n",
    "        # # initialize segments to empty list as this will store our\n",
    "        # # segmented signals \n",
    "        # segments = []\n",
    "        # labels = []\n",
    "\n",
    "        # # fig = plt.figure(figsize=(17, 5))\n",
    "        # n_frames = 0\n",
    "\n",
    "        # # this segments our signals into overlapping segments\n",
    "        # for i in range(0, (n_rows - samples_per_win_size) + samples_per_hop_size, samples_per_hop_size):\n",
    "        #     # # last segment would have start x: 464000 - end x: 512000\n",
    "        #     # # and because 512000 plus our hop size of 16000 = 528000 \n",
    "        #     # # already exceeding 521216 this then terminates the loop\n",
    "        #     # i += samples_per_hop_size\n",
    "        #     # start = i\n",
    "        #     # end = i + samples_per_win_size\n",
    "        #     start = i\n",
    "        #     end = min((i + samples_per_win_size), n_rows)\n",
    "        #     # print(f'start x: {start} - end x: {end}')\n",
    "\n",
    "        #     # extract segment from calculated start and end\n",
    "        #     # indeces\n",
    "        #     segment = x_signals[start:end]\n",
    "\n",
    "        #     # # calculate frequency domain features\n",
    "        #     # # get the spectrogram by calculating short time fourier transform\n",
    "        #     # spectrogram = np.abs(librosa.stft(segment))\n",
    "        #     # # print(f\"spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "        #     # # Get the frequencies corresponding to the spectrogram bins\n",
    "        #     # frequencies = librosa.fft_frequencies(sr=hertz)\n",
    "        #     # # print(f\"frequencies shape: {frequencies.shape}\")\n",
    "\n",
    "        #     # # Find the frequency bin with the highest average energy\n",
    "        #     # peak_frequency_bin = np.argmax(np.mean(spectrogram, axis=1))\n",
    "\n",
    "        #     # # Get the peak frequency in Hz\n",
    "        #     # # calculate also peak frequency\n",
    "        #     # # I think dito na gagamit ng fast fourier transform\n",
    "        #     # # to obtain the frequency, or use some sort of function\n",
    "        #     # # to convert the raw audio signals into a spectogram\n",
    "        #     # peak_frequency = frequencies[peak_frequency_bin]\n",
    "\n",
    "        #     # # calculate the segments fast fourier transform\n",
    "        #     # ft = np.fft.fft(segment)\n",
    "\n",
    "        #     # # the fft vector can have negative or positive values\n",
    "        #     # # so to avoid negative values and just truly see the frequencies\n",
    "        #     # # of each segment we use its absolute values instead \n",
    "        #     # magnitude = np.abs(ft)\n",
    "        #     # mag_len = magnitude.shape[0]\n",
    "        #     # frequency = np.linspace(0, hertz, mag_len)\n",
    "\n",
    "        #     # calculate statistical features\n",
    "        #     # because the frequency for each segment is 16000hz we can divide\n",
    "        #     # it by 1000 to instead to get its kilo hertz alternative\n",
    "        #     mean_freq_kHz = np.mean(segment, axis=0)\n",
    "        #     median_freq_kHz = np.median(segment, axis=0)\n",
    "        #     std_freq = np.std(segment, axis=0)\n",
    "        #     mode_freq = mode(segment, axis=0)\n",
    "            \n",
    "        #     # min = np.min(segment, axis=0)\n",
    "\n",
    "        #     # calculate first quantile, third quantile, interquartile range\n",
    "        #     first_quartile_kHz = np.percentile(segment, 25) / 1000,\n",
    "        #     third_quartile_kHz = np.percentile(segment, 75) / 1000,\n",
    "        #     inter_quartile_range_kHz = (np.percentile(segment, 75) - np.percentile(segment, 25)) / 1000,\n",
    "\n",
    "        #     # compute morphological features\n",
    "        #     skewness = skew(segment)\n",
    "        #     kurtosis = kurt(segment)\n",
    "\n",
    "        #     # compute time domain features\n",
    "        #     amp_env = np.max(segment, axis=0)\n",
    "        #     rms = np.sqrt(np.sum(segment ** 2, axis=0) / samples_per_win_size)\n",
    "\n",
    "        #     features = {\n",
    "        #         # statistical features\n",
    "        #         \"mean_freq_kHz\": mean_freq_kHz,\n",
    "        #         \"median_freq_kHz\": median_freq_kHz,\n",
    "        #         \"std_freq\": std_freq,\n",
    "        #         \"mode_freq\": mode_freq[0],\n",
    "        #         'first_quartile_kHz': first_quartile_kHz[0],\n",
    "        #         'third_quartile_kHz': third_quartile_kHz[0],\n",
    "        #         'inter_quartile_range_kHz': inter_quartile_range_kHz[0],\n",
    "\n",
    "        #         # morphological features\n",
    "        #         \"skewness\": skewness,\n",
    "        #         \"kurtosis\": kurtosis,\n",
    "\n",
    "        #         # time domain features\n",
    "        #         \"amp_env\":amp_env,\n",
    "        #         \"rms\": rms,\n",
    "                \n",
    "        #         # frequency features\n",
    "        #         # \"peak_frequency\": peak_frequency,\n",
    "        #     }\n",
    "            \n",
    "        #     segments.append(features)\n",
    "        #     labels.append(label)\n",
    "            \n",
    "        #     n_frames += 1\n",
    "\n",
    "        # frames = range(n_frames)\n",
    "        # # print(f\"number of frames resulting from window size of {samples_per_win_size} and a hop size of {samples_per_hop_size} from audio signal frequency of {hertz}: {frames}\")\n",
    "\n",
    "        # time = librosa.frames_to_time(frames, hop_length=samples_per_hop_size)\n",
    "        # # print(f\"shape of time calculated from number of frames: {time.shape[0]}\\n\")\n",
    "        \n",
    "        # # calculate other features\n",
    "        # zcr = librosa.feature.zero_crossing_rate(y=x_signals, frame_length=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # mel_spect = librosa.feature.melspectrogram(y=x_signals, sr=hertz, n_fft=samples_per_win_size, hop_length=samples_per_hop_size, n_mels=90)\n",
    "        # mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "        # mean_mel = np.mean(mel_spect_db, axis=0)\n",
    "        # variance_mel = np.var(mel_spect_db, axis=0)\n",
    "\n",
    "        # spect_cent = librosa.feature.spectral_centroid(y=x_signals, sr=hertz, n_fft=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # # chroma_stft = librosa.feature.chroma_stft(y=x_signals, frame_length=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # # print(mel_spect.shape, spect_cent.shape, zcr.shape)\n",
    "        # # print(f\"mel spectrogram shape: {mel_spect.shape}\")\n",
    "\n",
    "        # # calculate the number of values we need to remove in the\n",
    "        # # feature vector librosa calculated for us compared to the\n",
    "        # # feature vectors we calculated on our own\n",
    "        # zcr_n_values_to_rem = np.abs(zcr.shape[1] - time.shape[0])\n",
    "        # mean_mel_n_values_to_rem = np.abs(mean_mel.shape[0] - time.shape[0])\n",
    "        # spect_cent_n_values_to_rem = np.abs(spect_cent.shape[1] - time.shape[0])\n",
    "\n",
    "        # # get slice of those in range with time only\n",
    "        # zcr = zcr.reshape(-1)[:-zcr_n_values_to_rem]\n",
    "        # mean_mel = mean_mel.reshape(-1)[:-mean_mel_n_values_to_rem]\n",
    "        # variance_mel = variance_mel.reshape(-1)[:-mean_mel_n_values_to_rem]\n",
    "        # spect_cent = spect_cent.reshape(-1)[:-spect_cent_n_values_to_rem]\n",
    "\n",
    "        # # create features dataframe\n",
    "        # subject_features = pd.DataFrame.from_records(segments)\n",
    "        # subject_features[\"zcr\"] = zcr\n",
    "        # subject_features[\"mean_mel\"] = mean_mel\n",
    "        # subject_features[\"variance_mel\"] = variance_mel\n",
    "        # subject_features[\"spect_cent\"] = spect_cent\n",
    "        \n",
    "        # # create labels dataframe\n",
    "        # subject_labels = pd.Series(labels)\n",
    "\n",
    "        # os.makedirs(f\"./data/_EXTRACTED_FEATURES/{split}\", exist_ok=True)\n",
    "        # subject_features.to_csv(f'./data/_EXTRACTED_FEATURES/{split}/{name}_features.csv')\n",
    "        # subject_labels.to_csv(f'./data/_EXTRACTED_FEATURES/{split}/{name}_labels.csv')\n",
    "\n",
    "        # return (subject_features, subject_labels, name, time)\n",
    "        return subject_id, signal_df\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as exe: \n",
    "        signals_df = list(exe.map(helper, dataset))\n",
    "\n",
    "        # # unzip subjects data and unpack\n",
    "        # subjects_features, subjects_labels, subjects_names, time = zip(*subjects_data)\n",
    "    \n",
    "    # return subjects_features, subjects_labels, subjects_names, time\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4f57b764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f04b2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df = extract_features(dataset=train_signals_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "63c697cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[subjectId: string, signals: float, ID: int, freq_std: double, freq_skew: double, freq_kurt: double, freq_mean: double, freq_median: double, freq_mode: float, freq_min: float, freq_max: float, freq_var: double, freq_first_quart: double, freq_third_quart: double, freq_inter_quart_range: double]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals_df[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a78d6770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---+\n",
      "|freq_inter_quart_range| ID|\n",
      "+----------------------+---+\n",
      "|                   0.0|  1|\n",
      "|     -4.57763671875E-5|  2|\n",
      "|     -7.62939453125E-5|  3|\n",
      "|       -6.103515625E-5|  4|\n",
      "|       -1.220703125E-4|  5|\n",
      "|    -1.373291015625E-4|  6|\n",
      "|    -4.425048828125E-4|  7|\n",
      "|   -7.4005126953125E-4|  8|\n",
      "|        -7.32421875E-4|  9|\n",
      "|     -7.01904296875E-4| 10|\n",
      "|    -6.866455078125E-4| 11|\n",
      "|   -7.2479248046875E-4| 12|\n",
      "|     -8.85009765625E-4| 13|\n",
      "|  -0.00106048583984375| 14|\n",
      "|    -0.001129150390625| 15|\n",
      "|     -0.00115966796875| 16|\n",
      "|       -0.001220703125| 17|\n",
      "|        -0.00146484375| 18|\n",
      "|   -0.0014801025390625| 19|\n",
      "|  -0.00138092041015625| 20|\n",
      "+----------------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signals_df[0][-1].select(\"freq_inter_quart_range\", \"ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64fd942",
   "metadata": {},
   "outputs": [],
   "source": [
    "signals_df[0][-1]\n",
    "F.wind"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
