{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9057efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import io\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, FloatType, DoubleType, IntegerType\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from utilities.loaders import save_data_splits\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f65cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `sparksession is none: typeerror: 'javapackage' object is not \n",
    "# callable` can be raised if the pyspark version being used is 4.0.0\n",
    "# which is not compatible to a python 3.11.8 version\n",
    "spark = SparkSession.builder.appName(\"app\")\\\n",
    "    .config(\"spark.driver.memory\", \"14g\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\")\\\n",
    "    .config(\"spark.executor.cores\", \"6\")\\\n",
    "    .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01826ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver\\\\stage-01'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-01\"\n",
    "# SILVER_DATA_PATH = os.path.join(URL.format(FOLDER_NAME=SILVER_FOLDER_NAME), SUB_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "# folder_infos = dbutils.fs.ls(BRONZE_DATA_PATH)\n",
    "\n",
    "DATA_PATH = \"../include/data/\"\n",
    "SILVER_FOLDER_NAME = \"silver\"\n",
    "SUB_FOLDER_NAME = \"stage-01\"\n",
    "SILVER_DATA_PATH = os.path.join(DATA_PATH, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME))\n",
    "SILVER_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14e875b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"train\", \"labels.parquet\"))\n",
    "val_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"validate\", \"labels.parquet\"))\n",
    "test_labels_df = spark.read.format(\"parquet\").load(os.path.join(SILVER_DATA_PATH, \"test\", \"labels.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1956c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| value|            filePath|           subjectId|\n",
      "+------+--------------------+--------------------+\n",
      "|  male|file:///c:/Users/...|23yipikaye-201008...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...| 1337ad-20170321-tkg|\n",
      "|  male|file:///c:/Users/...| 1snoke-20120412-hge|\n",
      "|  male|file:///c:/Users/...|  Aaron-20080318-kdl|\n",
      "|  male|file:///c:/Users/...|   1028-20100710-hne|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3449b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n",
      "| value|            filePath|          subjectId|\n",
      "+------+--------------------+-------------------+\n",
      "|female|file:///c:/Users/...|1337ad-20170321-ajg|\n",
      "|  male|file:///c:/Users/...| Coren-20141121-pxp|\n",
      "+------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f1555b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| value|            filePath|           subjectId|\n",
      "+------+--------------------+--------------------+\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_labels_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b467ef",
   "metadata": {},
   "source": [
    "#### we will load the data in this format for concurrent processing and to prevent bottle neck issues of having to read files into one dataframe and then just to only convert it back to a array fo tuples with subject name and a corresponding spark dataframe. So why not instead read the parquet files like this?\n",
    "```\n",
    "[\n",
    "  (<subject 1>, <subject 1 spark df>),\n",
    "  (<subject 2>, <subject 2 spark df>),\n",
    "  (<subject 3>, <subject 3 spark df>),\n",
    "  ...\n",
    "  (<subject n>, <subject n spark df>),\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b444eb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_signal_files(SPLIT_FOLDER):\n",
    "\n",
    "    # only include the parquet files without labels\n",
    "    signal_files = [\n",
    "        os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE).replace('\\\\', '/') for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER) if not \"labels\" in SIGNAL_DF_FILE]\n",
    "\n",
    "    def helper(signal_file):\n",
    "        subject_id = signal_file.split('/')[-1].replace(\"_signals.parquet\", \"\")\n",
    "        signal_df = spark.read.format(\"parquet\").load(signal_file)\n",
    "        signal_df = signal_df.orderBy(\"ID\")\n",
    "        return subject_id, signal_df\n",
    "\n",
    "    with ThreadPoolExecutor() as exe:\n",
    "        signals_df = list(exe.map(helper, signal_files))\n",
    "\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd9bf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_URL = os.path.join(SILVER_DATA_PATH, \"{SPLIT}\")\n",
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"train\")\n",
    "train_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "910f5493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('1337ad-20170321-tkg',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('1snoke-20120412-hge',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('23yipikaye-20100807-ujm',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Aaron-20080318-kdl', DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-bft',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-cqj',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-hns',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Anniepoo-20140308-nky',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "093b15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"validate\")\n",
    "val_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f6e6b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1337ad-20170321-ajg',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int]),\n",
       " ('Coren-20141121-pxp', DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175ea18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"test\")\n",
    "test_signals_df = read_signal_files(SPLIT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "729c890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------+---+\n",
      "|        subjectId|      signals| ID|\n",
      "+-----------------+-------------+---+\n",
      "|1028-20100710-hne|-0.0054626465|  0|\n",
      "|1028-20100710-hne|-0.0053710938|  1|\n",
      "|1028-20100710-hne|-0.0055236816|  2|\n",
      "|1028-20100710-hne| -0.005493164|  3|\n",
      "|1028-20100710-hne| -0.005340576|  4|\n",
      "|1028-20100710-hne|-0.0047302246|  5|\n",
      "|1028-20100710-hne| -0.004486084|  6|\n",
      "|1028-20100710-hne|-0.0047302246|  7|\n",
      "|1028-20100710-hne|-0.0049438477|  8|\n",
      "|1028-20100710-hne| -0.004760742|  9|\n",
      "|1028-20100710-hne|-0.0042419434| 10|\n",
      "|1028-20100710-hne| -0.004211426| 11|\n",
      "|1028-20100710-hne| -0.004119873| 12|\n",
      "|1028-20100710-hne|-0.0032348633| 13|\n",
      "|1028-20100710-hne|-0.0029907227| 14|\n",
      "|1028-20100710-hne|-0.0032958984| 15|\n",
      "|1028-20100710-hne|-0.0035095215| 16|\n",
      "|1028-20100710-hne|-0.0036621094| 17|\n",
      "|1028-20100710-hne|-0.0036621094| 18|\n",
      "|1028-20100710-hne| -0.003967285| 19|\n",
      "+-----------------+-------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df[0][-1].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ccd87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LARRY\\anaconda3\\envs\\tech-interview\\Lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define a UDF to load audio with librosa\n",
    "@F.udf(returnType=ArrayType(FloatType()))\n",
    "def load_audio_with_librosa(content):\n",
    "    if content is None:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # Create a file-like object from the binary content \n",
    "        # which spark.read.format(\"binaryFile\").load(\"<path>\")\n",
    "        # returns\n",
    "        audio_buffer = io.BytesIO(content)\n",
    "\n",
    "        # we convert this audio buffer array as audio using librosa\n",
    "        # sr=None to preserve original sample rate\n",
    "        y, sr = librosa.load(audio_buffer, sr=16000) \n",
    "        \n",
    "        # top_db is set to 20 representing any signal below\n",
    "        # 20 decibels will be considered silence\n",
    "        y_trimmed, _ = librosa.effects.trim(y, top_db=20)\n",
    "\n",
    "        # Convert numpy array to list for Spark dataframe\n",
    "        return y_trimmed.tolist()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "@F.pandas_udf(returnType=FloatType(), functionType=F.PandasUDFType.GROUPED_AGG)\n",
    "def get_peak_freq(segment: pd.Series):\n",
    "    # calculate frequency domain features\n",
    "    # get the spectrogram by calculating short time fourier transform\n",
    "    spectrogram = np.abs(librosa.stft(segment))\n",
    "    # print(f\"spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "    # Get the frequencies corresponding to the spectrogram bins\n",
    "    frequencies = librosa.fft_frequencies(sr=16000)\n",
    "    # print(f\"frequencies shape: {frequencies.shape}\")\n",
    "\n",
    "    # Find the frequency bin with the highest average energy\n",
    "    peak_frequency_bin = np.argmax(np.mean(spectrogram, axis=1))\n",
    "\n",
    "    # Get the peak frequency in Hz\n",
    "    # calculate also peak frequency\n",
    "    # I think dito na gagamit ng fast fourier transform\n",
    "    # to obtain the frequency, or use some sort of function\n",
    "    # to convert the raw audio signals into a spectogram\n",
    "    peak_frequency = frequencies[peak_frequency_bin]\n",
    "\n",
    "    return peak_frequency\n",
    "\n",
    "def extract_features(dataset: list[tuple[str, pyspark.sql.DataFrame]], hertz: int=16000, window_time: int=3, hop_time: int=1):\n",
    "    \"\"\"\n",
    "    extracts the features from each segment of an audio signal\n",
    "\n",
    "    args:\n",
    "        dataset - \n",
    "        hertz - number of samples per second\n",
    "        window_time - number of seconds of the given window to consider\n",
    "        e.g. if number of seconds is 3 and hertz is 16000 or 16000\n",
    "        samples/rows per second then the window size we will consider\n",
    "        is 16000 * 3 or 48000\n",
    "        hop_time - seconds\n",
    "    \"\"\"\n",
    "    \n",
    "    def helper(datum):\n",
    "        # we access the SCR values via raw data column\n",
    "        subject_id = datum[0]\n",
    "        signal_df = datum[1]\n",
    "\n",
    "        # # get number of rows of 16000hz signals \n",
    "        # n_rows = x_signals.shape[0]\n",
    "        # # print(n_rows)\n",
    "\n",
    "        # we calculate the window size of each segment or the\n",
    "        # amount of samples it has to have based on the frequency\n",
    "        samples_per_win_size = int(window_time * hertz)\n",
    "        samples_per_hop_size = int(hop_time * hertz)\n",
    "        # print(f\"samples per window size: {samples_per_win_size}\")\n",
    "        # print(f\"samples per hop size: {samples_per_hop_size}\\n\")\n",
    "\n",
    "        \n",
    "        feat_window = Window.orderBy(\"ID\").rowsBetween(Window.currentRow, samples_per_win_size - 1)\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_skew\", F.skewness(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_kurt\", F.kurtosis(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_mean\", F.mean(\"signals\").over(feat_window))\n",
    "        # signal_df = signal_df.withColumn(\"freq_median\", F.median(\"signals\").over(feat_window))\n",
    "        # median over window function is not supported so we can use \n",
    "        signal_df = signal_df.withColumn(\"freq_median\", F.percentile(\"signals\", 0.5).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_mode\", F.mode(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_min\", F.min(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_max\", F.max(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_range\", F.col(\"freq_max\") - F.col(\"freq_min\"))\n",
    "        signal_df = signal_df.withColumn(\"freq_var\", F.variance(\"signals\").over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_std\", F.stddev(\"signals\").over(feat_window))\n",
    "        \n",
    "        signal_df = signal_df.withColumn(\"freq_first_quart\", F.percentile(\"signals\", 0.25).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_third_quart\", F.percentile(\"signals\", 0.75).over(feat_window))\n",
    "        signal_df = signal_df.withColumn(\"freq_inter_quart_range\", F.col(\"freq_first_quart\") - F.col(\"freq_third_quart\"))\n",
    "\n",
    "        # signal_df = signal_df.withColumn(\"freq_peak\", get_peak_freq(F.col(\"signals\")).over(feat_window))\n",
    "        \n",
    "        # an implementation of the only including windows after a certain\n",
    "        # hop size, since we cannot do it directly using spark we can \n",
    "        # filter out the rows of windows that have not yet made the \n",
    "        # appropriate hop size using filtering \n",
    "        signal_df = signal_df.where((F.col(\"ID\") % samples_per_hop_size) == 0)\n",
    "\n",
    "        # # initialize segments to empty list as this will store our\n",
    "        # # segmented signals \n",
    "        # segments = []\n",
    "        # labels = []\n",
    "\n",
    "        # # fig = plt.figure(figsize=(17, 5))\n",
    "        # n_frames = 0\n",
    "\n",
    "        # # this segments our signals into overlapping segments\n",
    "        # for i in range(0, (n_rows - samples_per_win_size) + samples_per_hop_size, samples_per_hop_size):\n",
    "        #     # # last segment would have start x: 464000 - end x: 512000\n",
    "        #     # # and because 512000 plus our hop size of 16000 = 528000 \n",
    "        #     # # already exceeding 521216 this then terminates the loop\n",
    "        #     # i += samples_per_hop_size\n",
    "        #     # start = i\n",
    "        #     # end = i + samples_per_win_size\n",
    "        #     start = i\n",
    "        #     end = min((i + samples_per_win_size), n_rows)\n",
    "        #     # print(f'start x: {start} - end x: {end}')\n",
    "\n",
    "        #     # extract segment from calculated start and end\n",
    "        #     # indeces\n",
    "        #     segment = x_signals[start:end]\n",
    "\n",
    "        #     # # calculate frequency domain features\n",
    "        #     # # get the spectrogram by calculating short time fourier transform\n",
    "        #     # spectrogram = np.abs(librosa.stft(segment))\n",
    "        #     # # print(f\"spectrogram shape: {spectrogram.shape}\")\n",
    "\n",
    "        #     # # Get the frequencies corresponding to the spectrogram bins\n",
    "        #     # frequencies = librosa.fft_frequencies(sr=hertz)\n",
    "        #     # # print(f\"frequencies shape: {frequencies.shape}\")\n",
    "\n",
    "        #     # # Find the frequency bin with the highest average energy\n",
    "        #     # peak_frequency_bin = np.argmax(np.mean(spectrogram, axis=1))\n",
    "\n",
    "        #     # # Get the peak frequency in Hz\n",
    "        #     # # calculate also peak frequency\n",
    "        #     # # I think dito na gagamit ng fast fourier transform\n",
    "        #     # # to obtain the frequency, or use some sort of function\n",
    "        #     # # to convert the raw audio signals into a spectogram\n",
    "        #     # peak_frequency = frequencies[peak_frequency_bin]\n",
    "\n",
    "        #     # # calculate the segments fast fourier transform\n",
    "        #     # ft = np.fft.fft(segment)\n",
    "\n",
    "        #     # # the fft vector can have negative or positive values\n",
    "        #     # # so to avoid negative values and just truly see the frequencies\n",
    "        #     # # of each segment we use its absolute values instead \n",
    "        #     # magnitude = np.abs(ft)\n",
    "        #     # mag_len = magnitude.shape[0]\n",
    "        #     # frequency = np.linspace(0, hertz, mag_len)\n",
    "\n",
    "        #     # calculate statistical features\n",
    "        #     # because the frequency for each segment is 16000hz we can divide\n",
    "        #     # it by 1000 to instead to get its kilo hertz alternative\n",
    "        #     mean_freq_kHz = np.mean(segment, axis=0)\n",
    "        #     median_freq_kHz = np.median(segment, axis=0)\n",
    "        #     std_freq = np.std(segment, axis=0)\n",
    "        #     mode_freq = mode(segment, axis=0)\n",
    "            \n",
    "        #     # min = np.min(segment, axis=0)\n",
    "\n",
    "        #     # calculate first quantile, third quantile, interquartile range\n",
    "        #     first_quartile_kHz = np.percentile(segment, 25) / 1000,\n",
    "        #     third_quartile_kHz = np.percentile(segment, 75) / 1000,\n",
    "        #     inter_quartile_range_kHz = (np.percentile(segment, 75) - np.percentile(segment, 25)) / 1000,\n",
    "\n",
    "        #     # compute morphological features\n",
    "        #     skewness = skew(segment)\n",
    "        #     kurtosis = kurt(segment)\n",
    "\n",
    "        #     # compute time domain features\n",
    "        #     amp_env = np.max(segment, axis=0)\n",
    "        #     rms = np.sqrt(np.sum(segment ** 2, axis=0) / samples_per_win_size)\n",
    "\n",
    "        #     features = {\n",
    "        #         # statistical features\n",
    "        #         \"mean_freq_kHz\": mean_freq_kHz,\n",
    "        #         \"median_freq_kHz\": median_freq_kHz,\n",
    "        #         \"std_freq\": std_freq,\n",
    "        #         \"mode_freq\": mode_freq[0],\n",
    "        #         'first_quartile_kHz': first_quartile_kHz[0],\n",
    "        #         'third_quartile_kHz': third_quartile_kHz[0],\n",
    "        #         'inter_quartile_range_kHz': inter_quartile_range_kHz[0],\n",
    "\n",
    "        #         # morphological features\n",
    "        #         \"skewness\": skewness,\n",
    "        #         \"kurtosis\": kurtosis,\n",
    "\n",
    "        #         # time domain features\n",
    "        #         \"amp_env\":amp_env,\n",
    "        #         \"rms\": rms,\n",
    "                \n",
    "        #         # frequency features\n",
    "        #         # \"peak_frequency\": peak_frequency,\n",
    "        #     }\n",
    "            \n",
    "        #     segments.append(features)\n",
    "        #     labels.append(label)\n",
    "            \n",
    "        #     n_frames += 1\n",
    "\n",
    "        # frames = range(n_frames)\n",
    "        # # print(f\"number of frames resulting from window size of {samples_per_win_size} and a hop size of {samples_per_hop_size} from audio signal frequency of {hertz}: {frames}\")\n",
    "\n",
    "        # time = librosa.frames_to_time(frames, hop_length=samples_per_hop_size)\n",
    "        # # print(f\"shape of time calculated from number of frames: {time.shape[0]}\\n\")\n",
    "        \n",
    "        # # calculate other features\n",
    "        # zcr = librosa.feature.zero_crossing_rate(y=x_signals, frame_length=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # mel_spect = librosa.feature.melspectrogram(y=x_signals, sr=hertz, n_fft=samples_per_win_size, hop_length=samples_per_hop_size, n_mels=90)\n",
    "        # mel_spect_db = librosa.power_to_db(mel_spect, ref=np.max)\n",
    "        # mean_mel = np.mean(mel_spect_db, axis=0)\n",
    "        # variance_mel = np.var(mel_spect_db, axis=0)\n",
    "\n",
    "        # spect_cent = librosa.feature.spectral_centroid(y=x_signals, sr=hertz, n_fft=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # # chroma_stft = librosa.feature.chroma_stft(y=x_signals, frame_length=samples_per_win_size, hop_length=samples_per_hop_size)\n",
    "        # # print(mel_spect.shape, spect_cent.shape, zcr.shape)\n",
    "        # # print(f\"mel spectrogram shape: {mel_spect.shape}\")\n",
    "\n",
    "        # # calculate the number of values we need to remove in the\n",
    "        # # feature vector librosa calculated for us compared to the\n",
    "        # # feature vectors we calculated on our own\n",
    "        # zcr_n_values_to_rem = np.abs(zcr.shape[1] - time.shape[0])\n",
    "        # mean_mel_n_values_to_rem = np.abs(mean_mel.shape[0] - time.shape[0])\n",
    "        # spect_cent_n_values_to_rem = np.abs(spect_cent.shape[1] - time.shape[0])\n",
    "\n",
    "        # # get slice of those in range with time only\n",
    "        # zcr = zcr.reshape(-1)[:-zcr_n_values_to_rem]\n",
    "        # mean_mel = mean_mel.reshape(-1)[:-mean_mel_n_values_to_rem]\n",
    "        # variance_mel = variance_mel.reshape(-1)[:-mean_mel_n_values_to_rem]\n",
    "        # spect_cent = spect_cent.reshape(-1)[:-spect_cent_n_values_to_rem]\n",
    "\n",
    "        # # create features dataframe\n",
    "        # subject_features = pd.DataFrame.from_records(segments)\n",
    "        # subject_features[\"zcr\"] = zcr\n",
    "        # subject_features[\"mean_mel\"] = mean_mel\n",
    "        # subject_features[\"variance_mel\"] = variance_mel\n",
    "        # subject_features[\"spect_cent\"] = spect_cent\n",
    "        \n",
    "        # # create labels dataframe\n",
    "        # subject_labels = pd.Series(labels)\n",
    "\n",
    "        # os.makedirs(f\"./data/_EXTRACTED_FEATURES/{split}\", exist_ok=True)\n",
    "        # subject_features.to_csv(f'./data/_EXTRACTED_FEATURES/{split}/{name}_features.csv')\n",
    "        # subject_labels.to_csv(f'./data/_EXTRACTED_FEATURES/{split}/{name}_labels.csv')\n",
    "\n",
    "        # return (subject_features, subject_labels, name, time)\n",
    "        return subject_id, signal_df\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as exe: \n",
    "        signals_df = list(exe.map(helper, dataset))\n",
    "\n",
    "        # # unzip subjects data and unpack\n",
    "        # subjects_features, subjects_labels, subjects_names, time = zip(*subjects_data)\n",
    "    \n",
    "    # return subjects_features, subjects_labels, subjects_names, time\n",
    "    return signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f57b764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne', DataFrame[subjectId: string, signals: float, ID: int])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f04b2f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_signals_df = extract_features(dataset=train_signals_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c34af99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1028-20100710-hne',\n",
       "  DataFrame[subjectId: string, signals: float, ID: int, freq_skew: double, freq_kurt: double, freq_mean: double, freq_median: double, freq_mode: float, freq_min: float, freq_max: float, freq_range: float, freq_var: double, freq_std: double, freq_first_quart: double, freq_third_quart: double, freq_inter_quart_range: double])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c697cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[subjectId: string, signals: float, ID: int, freq_skew: double, freq_kurt: double, freq_mean: double, freq_median: double, freq_mode: float, freq_min: float, freq_max: float, freq_range: float, freq_var: double, freq_std: double, freq_first_quart: double, freq_third_quart: double, freq_inter_quart_range: double]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_signals_df[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a78d6770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_signals_df[0][-1].select(\"freq_min\", \"ID\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95be1e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver\\\\stage-02'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-02\"\n",
    "# SILVER_DATA_PATH = URL.format(FOLDER_NAME=os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME))\n",
    "# SILVER_DATA_PATH\n",
    "SILVER_FOLDER_NAME = \"silver\"\n",
    "SUB_FOLDER_NAME = \"stage-02\"\n",
    "SILVER_DATA_PATH = os.path.join(DATA_PATH, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME))\n",
    "SILVER_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving 1028-20100710-hne signals...\n"
     ]
    }
   ],
   "source": [
    "save_data_splits(train_signals_df, split=\"train\", type_=\"signals\", save_path=SILVER_DATA_PATH)\n",
    "# save_data_splits(val_signals_df, split=\"validate\", type_=\"signals\", save_path=SILVER_DATA_PATH)\n",
    "# save_data_splits(test_signals_df, split=\"test\", type_=\"signals\", save_path=SILVER_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499a9f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [(i, (i + 1) * 10) for i in range(20)]\n",
    "df = spark.createDataFrame(sample_data, [\"row_id\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3110cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|row_id|value|\n",
      "+------+-----+\n",
      "|     0|   10|\n",
      "|     1|   20|\n",
      "|     2|   30|\n",
      "|     3|   40|\n",
      "|     4|   50|\n",
      "|     5|   60|\n",
      "|     6|   70|\n",
      "|     7|   80|\n",
      "|     8|   90|\n",
      "|     9|  100|\n",
      "|    10|  110|\n",
      "|    11|  120|\n",
      "|    12|  130|\n",
      "|    13|  140|\n",
      "|    14|  150|\n",
      "|    15|  160|\n",
      "|    16|  170|\n",
      "|    17|  180|\n",
      "|    18|  190|\n",
      "|    19|  200|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74857d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_win_size = 6\n",
    "samples_per_hop_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e73bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_window = Window.orderBy(\"row_id\").rowsBetween(Window.currentRow, samples_per_win_size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415a6fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|row_id|value|freq_std|\n",
      "+------+-----+--------+\n",
      "|     0|   10|     210|\n",
      "|     1|   20|     270|\n",
      "|     2|   30|     330|\n",
      "|     3|   40|     390|\n",
      "|     4|   50|     450|\n",
      "|     5|   60|     510|\n",
      "|     6|   70|     570|\n",
      "|     7|   80|     630|\n",
      "|     8|   90|     690|\n",
      "|     9|  100|     750|\n",
      "|    10|  110|     810|\n",
      "|    11|  120|     870|\n",
      "|    12|  130|     930|\n",
      "|    13|  140|     990|\n",
      "|    14|  150|    1050|\n",
      "|    15|  160|     900|\n",
      "|    16|  170|     740|\n",
      "|    17|  180|     570|\n",
      "|    18|  190|     390|\n",
      "|    19|  200|     200|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"freq_std\", F.sum(\"value\").over(feat_window))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b230ed",
   "metadata": {},
   "source": [
    "# an implementation of the only including windows after a certain hop size, since we cannot do it directly using spark we can filter out the rows of windows that have not yet made the appropriate hop size using filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87f04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------+\n",
      "|row_id|value|freq_std|\n",
      "+------+-----+--------+\n",
      "|     0|   10|     210|\n",
      "|     4|   50|     450|\n",
      "|     8|   90|     690|\n",
      "|    12|  130|     930|\n",
      "|    16|  170|     740|\n",
      "+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cond = ((F.col(\"row_id\") % samples_per_hop_size) == 0)\n",
    "df.where(cond).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
