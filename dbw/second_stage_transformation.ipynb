{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9057efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import io\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.conf import SparkConf\n",
    "# from pyspark.context import SparkContext\n",
    "from pyspark.sql.types import StringType, ArrayType, StructField, StructType, FloatType, DoubleType, IntegerType\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f65cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `sparksession is none: typeerror: 'javapackage' object is not \n",
    "# callable` can be raised if the pyspark version being used is 4.0.0\n",
    "# which is not compatible to a python 3.11.8 version\n",
    "spark = SparkSession.builder.appName(\"app\").getOrCreate()\n",
    "    # .config(\"spark.driver.memory\", \"14g\")\\\n",
    "    # .config(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"100\")\\\n",
    "    # .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c01826ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../include/data/silver\\\\stage-01'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL = \"abfss://{FOLDER_NAME}@sgppipelinesa.dfs.core.windows.net/\"\n",
    "# SILVER_FOLDER_NAME = \"sgppipelinesa-silver\"\n",
    "# SUB_FOLDER_NAME = \"stage-01\"\n",
    "# SILVER_DATA_PATH = os.path.join(URL.format(FOLDER_NAME=SILVER_FOLDER_NAME), SUB_FOLDER_NAME)\n",
    "# SILVER_DATA_PATH\n",
    "# folder_infos = dbutils.fs.ls(BRONZE_DATA_PATH)\n",
    "\n",
    "DATA_PATH = \"../include/data/\"\n",
    "SILVER_FOLDER_NAME = \"silver\"\n",
    "SUB_FOLDER_NAME = \"stage-01\"\n",
    "SILVER_DATA_PATH = os.path.join(DATA_PATH, os.path.join(SILVER_FOLDER_NAME, SUB_FOLDER_NAME))\n",
    "SILVER_DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e875b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df = spark.read.format(\"parquet\")\\\n",
    "    .load(os.path.join(SILVER_DATA_PATH, \"train\", \"labels.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1956c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| value|            filePath|           subjectId|\n",
      "+------+--------------------+--------------------+\n",
      "|  male|file:///c:/Users/...|23yipikaye-201008...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...|Anniepoo-20140308...|\n",
      "|female|file:///c:/Users/...| 1337ad-20170321-tkg|\n",
      "|  male|file:///c:/Users/...| 1snoke-20120412-hge|\n",
      "|  male|file:///c:/Users/...|  Aaron-20080318-kdl|\n",
      "|  male|file:///c:/Users/...|   1028-20100710-hne|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_labels_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9bf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_URL = os.path.join(SILVER_DATA_PATH, \"{SPLIT}\")\n",
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"train\")\n",
    "\n",
    "# only include the parquet files without labels\n",
    "train_signal_files = [os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE) for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER) if not \"labels\" in SIGNAL_DF_FILE]\n",
    "train_signals_df = spark.read.format(\"parquet\")\\\n",
    "    .load(train_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "910f5493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           subjectId|             signals|\n",
      "+--------------------+--------------------+\n",
      "|23yipikaye-201008...|[0.00491333, 0.00...|\n",
      "|Anniepoo-20140308...|[-5.187988E-4, -4...|\n",
      "|Anniepoo-20140308...|[0.012268066, 0.0...|\n",
      "| 1snoke-20120412-hge|[0.0014343262, 0....|\n",
      "|Anniepoo-20140308...|[-0.0045166016, -...|\n",
      "|   1028-20100710-hne|[-0.0054626465, -...|\n",
      "| 1337ad-20170321-tkg|[0.0015869141, 0....|\n",
      "|Anniepoo-20140308...|[1.8310547E-4, 7....|\n",
      "|  Aaron-20080318-kdl|[-2.4414062E-4, -...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_signals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "093b15c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"validate\")\n",
    "\n",
    "# only include the parquet files without labels\n",
    "val_signal_files = [os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE) for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER) if not \"labels\" in SIGNAL_DF_FILE]\n",
    "val_signals_df = spark.read.format(\"parquet\")\\\n",
    "    .load(val_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f6e6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|          subjectId|             signals|\n",
      "+-------------------+--------------------+\n",
      "| Coren-20141121-pxp|[-3.9672852E-4, 3...|\n",
      "|1337ad-20170321-ajg|[-0.0010375977, -...|\n",
      "+-------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_signals_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "175ea18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_FOLDER = SPLIT_URL.format(SPLIT=\"test\")\n",
    "\n",
    "# only include the parquet files without labels\n",
    "test_signal_files = [os.path.join(SPLIT_FOLDER, SIGNAL_DF_FILE) for SIGNAL_DF_FILE in os.listdir(SPLIT_FOLDER) if not \"labels\" in SIGNAL_DF_FILE]\n",
    "test_signals_df = spark.read.format(\"parquet\")\\\n",
    "    .load(test_signal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "729c890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|           subjectId|             signals|\n",
      "+--------------------+--------------------+\n",
      "|Anniepoo-20140308...|[0.0050354004, 0....|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_signals_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tech-interview",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
